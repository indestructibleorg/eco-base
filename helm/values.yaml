# SuperAI Platform - Helm Values
global:
  namespace: superai
  imageRegistry: ""
  imagePullSecrets: []

# API Gateway
api:
  replicaCount: 3
  image:
    repository: superai-platform/api
    tag: "1.0.0"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8000
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "2000m"
      memory: "4Gi"
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilization: 70
  env:
    SUPERAI_ENVIRONMENT: production
    SUPERAI_LOG_LEVEL: INFO
    SUPERAI_WORKERS: "4"

# vLLM Engine
vllm:
  enabled: true
  replicas: 1
  image:
    repository: vllm/vllm-openai
    tag: "v0.6.6"
  model: "meta-llama/Llama-3.1-8B-Instruct"
  gpuMemoryUtilization: "0.90"
  maxModelLen: "32768"
  tensorParallelSize: "1"
  enablePrefixCaching: true
  port: 8001
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: "1"
    limits:
      cpu: "8"
      memory: "32Gi"
      nvidia.com/gpu: "1"
  storage:
    size: 100Gi
    storageClass: fast-ssd

# SGLang Engine
sglang:
  enabled: true
  replicas: 1
  image:
    repository: lmsysorg/sglang
    tag: "v0.3.6-cu124"
  model: "meta-llama/Llama-3.1-8B-Instruct"
  memFractionStatic: "0.88"
  port: 8003
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: "1"
    limits:
      cpu: "8"
      memory: "32Gi"
      nvidia.com/gpu: "1"
  storage:
    size: 100Gi
    storageClass: fast-ssd

# TGI Engine
tgi:
  enabled: true
  replicas: 1
  image:
    repository: ghcr.io/huggingface/text-generation-inference
    tag: "2.4.1"
  model: "meta-llama/Llama-3.1-8B-Instruct"
  maxInputLength: 4096
  maxTotalTokens: 8192
  port: 8002
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: "1"
    limits:
      cpu: "8"
      memory: "32Gi"
      nvidia.com/gpu: "1"
  storage:
    size: 100Gi
    storageClass: fast-ssd

# Ollama Engine
ollama:
  enabled: false
  replicas: 1
  image:
    repository: ollama/ollama
    tag: "latest"
  port: 11434
  resources:
    requests:
      cpu: "2"
      memory: "8Gi"
      nvidia.com/gpu: "1"
    limits:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: "1"
  storage:
    size: 100Gi
    storageClass: fast-ssd

# Redis
redis:
  enabled: true
  image:
    repository: redis
    tag: "7-alpine"
  resources:
    requests:
      cpu: "250m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "3Gi"
  storage:
    size: 10Gi

# PostgreSQL
postgres:
  enabled: true
  image:
    repository: postgres
    tag: "16-alpine"
  resources:
    requests:
      cpu: "250m"
      memory: "512Mi"
    limits:
      cpu: "2000m"
      memory: "4Gi"
  storage:
    size: 50Gi

# Monitoring
monitoring:
  prometheus:
    enabled: true
    retention: 30d
    storage:
      size: 50Gi
  grafana:
    enabled: true
    adminPassword: ""  # Set via secret
    storage:
      size: 5Gi

# Ingress
ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: api.superai.platform
      paths:
        - path: /
          pathType: Prefix
          service: superai-api-svc
          port: 8000
  tls:
    - secretName: superai-tls
      hosts:
        - api.superai.platform

# Secrets (override via --set or external secret manager)
secrets:
  secretKey: ""
  postgresPassword: ""
  hfToken: ""